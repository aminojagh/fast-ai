{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aminojagh/fast-ai/blob/main/NB7-Stable_Diffusion_warmup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9bb34bd",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "c9bb34bd"
      },
      "source": [
        "# Stable Diffusion with ðŸ¤— Diffusers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uq diffusers transformers fastcore"
      ],
      "metadata": {
        "id": "eBaqX34Y93lt"
      },
      "id": "eBaqX34Y93lt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Stable Diffusion"
      ],
      "metadata": {
        "id": "IkHIzxML_gs9"
      },
      "id": "IkHIzxML_gs9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "To run Stable Diffusion on your computer you have to accept the model license. It's an open CreativeML OpenRail-M license that claims no rights on the outputs you generate and prohibits you from deliberately producing illegal or harmful content. The [model card](https://huggingface.co/CompVis/stable-diffusion-v1-4) provides more details. If you do accept the license, you need to be a registered user in ðŸ¤— Hugging Face Hub and use an access token for the code to work. You have two options to provide your access token:\n",
        "\n",
        "* Use the `huggingface-cli login` command-line tool in your terminal and paste your token when prompted. It will be saved in a file in your computer.\n",
        "* Or use `notebook_login()` in a notebook, which does the same thing."
      ],
      "metadata": {
        "id": "pTR5D-yR97XO"
      },
      "id": "pTR5D-yR97XO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "119badbb",
      "metadata": {
        "id": "119badbb"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from fastcore.all import concat\n",
        "from huggingface_hub import notebook_login\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logging.disable(logging.WARNING)\n",
        "torch.manual_seed(1)\n",
        "if not (Path.home()/'.cache/huggingface'/'token').exists(): notebook_login()"
      ],
      "metadata": {
        "id": "kp-6jlwA-_TE"
      },
      "id": "kp-6jlwA-_TE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stable Diffusion Pipeline"
      ],
      "metadata": {
        "id": "hJrHXLz0_QU1"
      },
      "id": "hJrHXLz0_QU1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "[`StableDiffusionPipeline`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion#diffusers.StableDiffusionPipeline) is an end-to-end [diffusion inference pipeline](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion) that allows you to start generating images with just a few lines of code. Many Hugging Face libraries (along with other libraries such as scikit-learn) use the concept of a \"pipeline\" to indicate a sequence of steps that when combined complete some task. We'll look at the individual steps of the pipeline later -- for now though, let's just use it to see what it can do.\n",
        "\n",
        "We use [`from_pretrained`](https://huggingface.co/docs/diffusers/main/en/api/diffusion_pipeline#diffusers.DiffusionPipeline.from_pretrained) to create the pipeline and download the pretrained weights. We indicate that we want to use the `fp16` (half-precision) version of the weights, and we tell `diffusers` to expect the weights in that format. This allows us to perform much faster inference with almost no discernible difference in quality.\n",
        "\n",
        "The string passed to `from_pretrained` in this case (`CompVis/stable-diffusion-v1-4`) is\n",
        "- the repo id of a pretrained pipeline hosted on [Hugging Face Hub](https://huggingface.co/models);\n",
        "- it can also be a path to a directory containing pipeline weights."
      ],
      "metadata": {
        "id": "tnU-xWly_kiQ"
      },
      "id": "tnU-xWly_kiQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0eb6cccd",
      "metadata": {
        "id": "0eb6cccd"
      },
      "outputs": [],
      "source": [
        "pipe = StableDiffusionPipeline.\\\n",
        "          from_pretrained(\n",
        "              \"CompVis/stable-diffusion-v1-4\",\n",
        "              variant=\"fp16\",\n",
        "              torch_dtype=torch.float16\n",
        "          )\\\n",
        "          .to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The weights for all the models in the pipeline will be downloaded\n",
        "# and cached the first time you run this cell.\n",
        "# The weights are cached in your home directory by default.\n",
        "!ls ~/.cache/huggingface/hub"
      ],
      "metadata": {
        "id": "7OxixLEY_YQO"
      },
      "id": "7OxixLEY_YQO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are now ready to use the pipeline to start creating images."
      ],
      "metadata": {
        "id": "DOgqV5pSBAPl"
      },
      "id": "DOgqV5pSBAPl"
    },
    {
      "cell_type": "code",
      "source": [
        "# # If your GPU is not big enough to use `pipe`,\n",
        "# # run `pipe.enable_attention_slicing()`\n",
        "# # As described in the docs:\n",
        "# #    When this option is enabled, the attention module will\n",
        "# #    split the input tensor in slices, to compute attention\n",
        "# #    in several steps. This is useful to save some memory\n",
        "# #    in exchange for a small speed decrease.\n",
        "\n",
        "# pipe.enable_attention_slicing()"
      ],
      "metadata": {
        "id": "17EF4q89BDlp"
      },
      "id": "17EF4q89BDlp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"a photograph of an astronaut riding a horse\"\n",
        "pipe(prompt).images[0]"
      ],
      "metadata": {
        "id": "RFUTp1UMBFzy"
      },
      "id": "RFUTp1UMBFzy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1024)\n",
        "pipe(prompt).images[0]"
      ],
      "metadata": {
        "id": "ii6hhOZhBRkX"
      },
      "id": "ii6hhOZhBRkX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will have noticed that running the pipeline shows a progress bar with a certain number of steps. This is because Stable Diffusion is based on a progressive denoising algorithm that is able to create a convincing image starting from pure random noise. Models in this family are known as _diffusion models_."
      ],
      "metadata": {
        "id": "pXYOv1YxBrbk"
      },
      "id": "pXYOv1YxBrbk"
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1024)\n",
        "pipe(prompt, num_inference_steps=3).images[0]"
      ],
      "metadata": {
        "id": "oVBPDIRNBXuQ"
      },
      "id": "oVBPDIRNBXuQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1024)\n",
        "pipe(prompt, num_inference_steps=16).images[0]"
      ],
      "metadata": {
        "id": "DCkV8bV9B3V0"
      },
      "id": "DCkV8bV9B3V0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classifier-Free Guidance"
      ],
      "metadata": {
        "id": "d8xgElfUCLWY"
      },
      "id": "d8xgElfUCLWY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "_Classifier-Free Guidance_ is a method to increase the adherence of the output to the conditioning signal we used (the text).\n",
        "\n",
        "Roughly speaking, the larger the guidance the more the model tries to represent the text prompt. However, large values tend to produce less diversity. The default is `7.5`, which represents a good compromise between variety and fidelity. This [blog post](https://benanne.github.io/2022/05/26/guidance.html) goes into deeper details on how it works."
      ],
      "metadata": {
        "id": "frACwnYpCLPF"
      },
      "id": "frACwnYpCLPF"
    },
    {
      "cell_type": "code",
      "source": [
        "def image_grid(imgs, rows, cols):\n",
        "    w,h = imgs[0].size\n",
        "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
        "    for i, img in enumerate(imgs): grid.paste(img, box=(i%cols*w, i//cols*h))\n",
        "    return grid"
      ],
      "metadata": {
        "id": "punGDLjvB3-p"
      },
      "id": "punGDLjvB3-p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_rows,num_cols = 4,4\n",
        "prompts = [prompt] * num_cols\n",
        "# We can generate multiple images for the same prompt\n",
        "# by simply passing a list of prompts instead of a string.\n",
        "images = concat(pipe(prompts, guidance_scale=g).images for g in [1.1,3,7,14])\n",
        "image_grid(images, rows=num_rows, cols=num_cols)"
      ],
      "metadata": {
        "id": "Zwad0lbwCwA5"
      },
      "id": "Zwad0lbwCwA5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Negative prompts"
      ],
      "metadata": {
        "id": "9eJiNQOwDD-q"
      },
      "id": "9eJiNQOwDD-q"
    },
    {
      "cell_type": "markdown",
      "source": [
        "_Negative prompting_ refers to the use of another prompt (instead of a completely unconditioned generation), and scaling the difference between generations of that prompt and the conditioned generation.\n",
        "\n",
        "By using the negative prompt we move more towards the direction of the positive prompt, effectively reducing the importance of the negative prompt in our composition."
      ],
      "metadata": {
        "id": "nls0DahHDH0O"
      },
      "id": "nls0DahHDH0O"
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1000)\n",
        "prompt = \"Labrador in the style of Vermeer\"\n",
        "img0 = pipe(prompt).images[0]\n",
        "\n",
        "torch.manual_seed(1000)\n",
        "img1 = pipe(prompt, negative_prompt=\"blue\").images[0]\n",
        "\n",
        "image_grid([img0, img1], rows=1, cols=2)"
      ],
      "metadata": {
        "id": "rTgOlelEDGAe"
      },
      "id": "rTgOlelEDGAe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Image to Image"
      ],
      "metadata": {
        "id": "8Rmbv4HGD4Di"
      },
      "id": "8Rmbv4HGD4Di"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even though Stable Diffusion was trained to generate images, and optionally drive the generation using text conditioning, we can use the raw image diffusion process for other tasks.\n",
        "\n",
        "For example, instead of starting from pure noise, we can start from an image an add a certain amount of noise to it. We are replacing the initial steps of the denoising and pretending our image is what the algorithm came up with. Then we continue the diffusion process from that state as usual. This usually preserves the composition although details may change a lot. It's great for sketches!"
      ],
      "metadata": {
        "id": "QA5gXZ3IEXyJ"
      },
      "id": "QA5gXZ3IEXyJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "These operations (provide an initial image, add some noise to it and run diffusion from there) can be automatically performed by a special image to image pipeline: `StableDiffusionImg2ImgPipeline`. This is the source code for its [`__call__` method](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py#L124)."
      ],
      "metadata": {
        "id": "kuHlAK1QDmrt"
      },
      "id": "kuHlAK1QDmrt"
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import StableDiffusionImg2ImgPipeline\n",
        "from fastdownload import FastDownload\n",
        "\n",
        "pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
        "    \"CompVis/stable-diffusion-v1-4\",\n",
        "    variant=\"fp16\",\n",
        "    torch_dtype=torch.float16,\n",
        ").to(\"cuda\")"
      ],
      "metadata": {
        "id": "AvIATC_UDf0a"
      },
      "id": "AvIATC_UDf0a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use as an example the following sketch created by [user VigilanteRogue81](https://huggingface.co/spaces/huggingface-projects/diffuse-the-rest/discussions/204)."
      ],
      "metadata": {
        "id": "DFxO4oG5EucZ"
      },
      "id": "DFxO4oG5EucZ"
    },
    {
      "cell_type": "code",
      "source": [
        "hf_cdn = 'https://cdn-uploads.huggingface.co'\n",
        "sketch_path = f'{hf_cdn}/production/uploads/1664665907257-noauth.png'\n",
        "p = FastDownload().download(sketch_path)\n",
        "init_image = Image.open(p).convert(\"RGB\")\n",
        "init_image"
      ],
      "metadata": {
        "id": "-SIzOziGFPjI"
      },
      "id": "-SIzOziGFPjI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1000)\n",
        "prompt = \"Wolf howling at the moon, photorealistic 4K\"\n",
        "images = pipe(prompt=prompt,\n",
        "              num_images_per_prompt=3,\n",
        "              image=init_image,\n",
        "              strength=0.8,\n",
        "              num_inference_steps=50).images\n",
        "image_grid(images, rows=1, cols=3)"
      ],
      "metadata": {
        "id": "ZACI8_4SEr9u"
      },
      "id": "ZACI8_4SEr9u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we get a composition we like we can use it as the next seed for another prompt and further change the results. For example, let's take the third image above and try to use it to generate something in the style of Van Gogh."
      ],
      "metadata": {
        "id": "qNqUupChFzix"
      },
      "id": "qNqUupChFzix"
    },
    {
      "cell_type": "code",
      "source": [
        "init_image = images[2]\n",
        "torch.manual_seed(1000)\n",
        "prompt = \"Oil painting of wolf howling at the moon by Van Gogh\"\n",
        "images = pipe(prompt=prompt,\n",
        "              num_images_per_prompt=3,\n",
        "              image=init_image,\n",
        "              strength=1,\n",
        "              num_inference_steps=70).images\n",
        "image_grid(images, rows=1, cols=3)"
      ],
      "metadata": {
        "id": "KF2n2ys1FFCw"
      },
      "id": "KF2n2ys1FFCw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creative people use different tools in a process of iterative refinement to come up with the ideas they have in mind. Here's a [list with some suggestions](https://github.com/fastai/diffusion-nbs/blob/43a090286e5742f807d4ff58524c02a1888b3004/suggested_tools.md) to get started."
      ],
      "metadata": {
        "id": "XKco9Dv2F_ah"
      },
      "id": "XKco9Dv2F_ah"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-tuning"
      ],
      "metadata": {
        "id": "Y_wjoO-wGikj"
      },
      "id": "Y_wjoO-wGikj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "[How we made the text-to-pokemon model at Lambda](https://lambdalabs.com/blog/how-to-fine-tune-stable-diffusion-how-we-made-the-text-to-pokemon-model-at-lambda/)\n",
        "\n",
        "\n",
        "![](https://lambdalabs.com/hs-fs/hubfs/2.%20Images/Images%20-%20Blog%20Posts/2022%20-%20Blog%20Images/image--3-.png?width=400&height=400&name=image--3-.png)\n",
        "\n",
        "Girl with a pearl earring, Cute Obama creature, Donald Trump, Boris Johnson, Totoro, Hello Kitty"
      ],
      "metadata": {
        "id": "6ZHUgOD0GwCr"
      },
      "id": "6ZHUgOD0GwCr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Textual Inversion"
      ],
      "metadata": {
        "id": "uI-KKAf_HgDW"
      },
      "id": "uI-KKAf_HgDW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Textual inversion is a process where you can quickly \"teach\" a new word to the text model and train its embeddings close to some visual representation. This is achieved by adding a new token to the vocabulary, freezing the weights of all the models (except the text encoder), and train with a few representative images.\n",
        "\n",
        "This is a schematic representation of the process by the [authors of the paper](https://textual-inversion.github.io).\n",
        "\n",
        "![Textual Inversion diagram](https://textual-inversion.github.io/static/images/training/training.JPG?width=300&height=300)"
      ],
      "metadata": {
        "id": "5ccy_vZYHWH1"
      },
      "id": "5ccy_vZYHWH1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can train your own tokens with photos you provide using [this training script](https://github.com/huggingface/diffusers/tree/main/examples/textual_inversion) or [Google Colab notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb). There's also a [Colab notebook for inference](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_conceptualizer_inference.ipynb), but we'll show below the steps we have to follow to add a trained token to the vocabulary and make it work the pre-trained Stable Diffusion model.\n",
        "\n",
        "We'll try an example using embeddings trained for [this style](https://huggingface.co/sd-concepts-library/indian-watercolor-portraits)."
      ],
      "metadata": {
        "id": "kX7U14agHwH2"
      },
      "id": "kX7U14agHwH2"
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = StableDiffusionPipeline\\\n",
        "          .from_pretrained(\n",
        "              \"CompVis/stable-diffusion-v1-4\",\n",
        "              variant=\"fp16\",\n",
        "              torch_dtype=torch.float16\n",
        "          ).to(\"cuda\")\n",
        "\n",
        "styles_repo = \"https://huggingface.co/sd-concepts-library\"\n",
        "style_name = \"indian-watercolor-portraits\"\n",
        "embeds_url = f\"{styles_repo}/{style_name}/resolve/main/learned_embeds.bin\"\n",
        "embeds_path = FastDownload().download(embeds_url)\n",
        "embeds_dict = torch.load(str(embeds_path), map_location=\"cpu\")"
      ],
      "metadata": {
        "id": "r8JbRNoeIOli"
      },
      "id": "r8JbRNoeIOli",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The embeddings for the new token are stored in a small PyTorch pickled dictionary, whose key is the new text token that was trained. Since the encoder of our pipeline does not know about this term, we need to manually append it."
      ],
      "metadata": {
        "id": "7KuBoLh7I1A3"
      },
      "id": "7KuBoLh7I1A3"
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = pipe.tokenizer\n",
        "text_encoder = pipe.text_encoder\n",
        "new_token, embeds = next(iter(embeds_dict.items()))\n",
        "embeds = embeds.to(text_encoder.dtype)\n",
        "print(new_token)\n",
        "print(embeds.shape)"
      ],
      "metadata": {
        "id": "sHMxC8mnJDN3"
      },
      "id": "sHMxC8mnJDN3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We add the new token to the tokenizer and the trained embeddings to the embeddings table."
      ],
      "metadata": {
        "id": "p10KCgrVJm-o"
      },
      "id": "p10KCgrVJm-o"
    },
    {
      "cell_type": "code",
      "source": [
        "assert tokenizer.add_tokens(new_token) == 1, \"The token already exists!\"\n",
        "text_encoder.resize_token_embeddings(len(tokenizer))\n",
        "new_token_id = tokenizer.convert_tokens_to_ids(new_token)\n",
        "text_encoder.get_input_embeddings().weight.data[new_token_id] = embeds"
      ],
      "metadata": {
        "id": "QkAnwZayIM55"
      },
      "id": "QkAnwZayIM55",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now run inference and refer to the style as if it was another word in the dictionary."
      ],
      "metadata": {
        "id": "AYrAb7Q2KNW3"
      },
      "id": "AYrAb7Q2KNW3"
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1000)\n",
        "image = pipe(\"Woman reading in the style of <watercolor-portrait>\").images[0]\n",
        "image"
      ],
      "metadata": {
        "id": "OOUAz3RJHotz"
      },
      "id": "OOUAz3RJHotz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dreambooth"
      ],
      "metadata": {
        "id": "s4At3B1rLe_f"
      },
      "id": "s4At3B1rLe_f"
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Dreambooth](https://dreambooth.github.io) is a kind of fine-tuning that attempts to introduce new subjects by providing just a few images of the new subject. The goal is similar to that of [Textual Inversion](#Textual-Inversion), but the process is different. Instead of creating a new token as Textual Inversion does, we select an existing token in the vocabulary (usually a rarely used one), and fine-tune the model for a few hundred steps to bring that token close to the images we provide. This is a regular fine-tuning process in which all modules are unfrozen.\n",
        "For example, we fine-tuned a model with a prompt like `\"photo of a sks person\"`, using the rare `sks` token to qualify the term `person`, and using photos of Jeremy as the targets. Let's see how it works."
      ],
      "metadata": {
        "id": "NZr8XwItLmda"
      },
      "id": "NZr8XwItLmda"
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = StableDiffusionPipeline\\\n",
        "        .from_pretrained(\n",
        "            \"pcuenq/jh_dreambooth_1000\",\n",
        "            torch_dtype=torch.float16\n",
        "        )\n",
        "pipe = pipe.to(\"cuda\")\n",
        "torch.manual_seed(1000)\n",
        "prompt = \"Painting of sks person in the style of Paul Signac\"\n",
        "images = pipe(prompt, num_images_per_prompt=4).images\n",
        "image_grid(images, 1, 4)"
      ],
      "metadata": {
        "id": "SzyEqpwCLkAb"
      },
      "id": "SzyEqpwCLkAb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine-tuning with Dreambooth is finicky and sensitive to hyperparameters, as we are essentially asking the model to overfit the prompt to the supplied images. In some situations we observe problems such as catastrophic forgetting of the associated term (`\"person\"` in this case). The authors applied a technique called \"prior preservation\" to try to avoid it by performing a special regularization using other examples of the term, in addition to the ones we provided. The cool thing about this idea is that those examples can be easily generated by the pre-trained Stable Diffusion model itself! We did not use that method in the model we trained for the previous example."
      ],
      "metadata": {
        "id": "n0pbGpYsLqxN"
      },
      "id": "n0pbGpYsLqxN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Other ideas that may work include: use EMA so that the final weights preserve some of the previous knowledge, use progressive learning rates for fine-tuning, or combine the best of Textual Inversion with Dreambooth. These could make for some interesting projects to try out!\n",
        "\n",
        "If you want to train your own Dreambooth model, you can use [this script](https://github.com/huggingface/diffusers/tree/main/examples/dreambooth) or [this Colab notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_training.ipynb)."
      ],
      "metadata": {
        "id": "IuPXBVpoMSP_"
      },
      "id": "IuPXBVpoMSP_"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qnSYBuRfLrXZ"
      },
      "id": "qnSYBuRfLrXZ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}